{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=black>Assignment 2: XOR & Multiplication NN Implementation</font>\n",
    "### <font color=black>Jarred van de Voort</font>\n",
    "\n",
    "#### <font color=black>Instructions: </font>\n",
    "1) The first case to study is the well known XOR example. For this example, the hidden\n",
    "layer maybe simply made of two units, N = 2, and the input and target output are\n",
    "binary values.\n",
    "There are only four different possible inputs/outputs scenarios, Thus, for the training set,\n",
    "validation set and generalization set, for each step τ we only have these four examples.\n",
    "Show the plots we described in visualization, show the final generalization loss, show\n",
    "all the weights of the network (list them). How to show the final generalization output\n",
    "? we should be able to run your forward code, with the final weights, and your generalization\n",
    "set, and produce your final outputs.\n",
    "\n",
    "2) The second example we ask you to run the multiplication function. The inputs are\n",
    "x1, x2 and they are real values. Well, let us restrict to multiplications of integers ? The\n",
    "target output should be t = x1 × x2 also an integer number. Note that it should work\n",
    "on negative numbers as well, i.e., x1, x2, t ∈ Z. We may restrict further to x1, x2, t ∈\n",
    "(−M, M), say M=100. It is easy to generate a large test set, not so large validation set,\n",
    "and an even larger generalization set. Say D = 1, 000, V = 200 and G = 4, 000. Of\n",
    "course, make sure to include cases of multiplication by zero and by 1.\n",
    "In this case, the hidden layer should be \"large\" to have a chance to work. Consider\n",
    "N = 20 (I am not sure it will work!). Display the same plots and values as the XOR\n",
    "problem and we should be able to run your forward code, with the final weights, and\n",
    "produce your final output.\n",
    "\n",
    "#### <font color=black>Note: </font>\n",
    "In the assignment, its specified that the network sjould multiply integers from -100 to 100. However, I've found that reLu has trouble with negative values, so I've opted to implement sigmoid activation. Likewise, I've decided to restrict values to 0,1 for simplicity & to demonstrate the capabilities of a trained NN. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=black>Activation Function: Sigmoid</font>\n",
    "The sigmoid function is an activation function that converts linear outputs to nonlinear outputs bounding the output between 0 & 1. We can also compute the gradient by taking the derivative of the sigmoid function which is useful for calculating error. I've found that using the sigmoid function is much easier to implement than the ReLu activation fuction, and appears to work well with both the XOR training set and multiplication training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(net_input):\n",
    "    return 1/(1+np.exp(-net_input))\n",
    "\n",
    "def sigmoid_grad(net_input):\n",
    "    s = sigmoid(net_input)\n",
    "    return s*(1-s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=black>Performing Forward Propagation</font>\n",
    "The function provides a way for the neural network to propogate through the network & compute the output of every layer with the given inputs and weights and do a forward pass to the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate(training_inputs,theta_1,theta_2):\n",
    "    \n",
    "    #We first add a set of biases to the input of the first layer\n",
    "    l1_input = np.c_[np.ones(training_inputs.shape[0]),training_inputs]#mxn+1\n",
    "    \n",
    "    #The input of the first layer is multiplied by the weights of the first layer\n",
    "    l1_output = l1_input.dot(theta_1)\n",
    "    \n",
    "    #To get the input of the second layer, we pass the output of the first layer through an activation function and factor in biases \n",
    "    l2_input = np.c_[np.ones(training_inputs.shape[0]),sigmoid(l1_output)]\n",
    "    \n",
    "    #The input of the second layer is multiplied by the weights\n",
    "    l2_output = l2_input.dot(theta_2)\n",
    "    \n",
    "    #The output is passed through the activation function to obtain the final probability\n",
    "    l3_output = sigmoid(l2_output)\n",
    "    \n",
    "    return l1_input,l1_output,l2_input,l2_output,l3_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=black>Performing Backpropagation</font>\n",
    "For each iteration/epoch, we compute the error of each layer and update the weights as to minimize error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagate(training_inputs, training_outputs, theta_1, theta_2, learning_rate, show_detailed):\n",
    "    \n",
    "    l1_input, l1_output, l2_input, l2_output, pred = forward_propagate(training_inputs, theta_1, theta_2)\n",
    "    \n",
    "    #Error is computed as the difference between the predicted output and actual output\n",
    "    del_2 = training_outputs - pred\n",
    "    error = (np.sum(np.absolute(del_2))) #To get a sense of how well the NN is performing we can plot error for each iter\n",
    "\n",
    "    #The error of the previous layer is found by computing the dot product of the error of the previous layer and the weights of the second layer\n",
    "    del_1=del_2.dot(theta_2[1:,:].T)\n",
    "    delta2 = del_2\n",
    "            \n",
    "    #The error of the first layer is multiplied by the sigmoid gradient of the output of the first layer\n",
    "    delta1 = del_1*sigmoid_grad(l1_output)\n",
    "        \n",
    "    #The parameters are updated using gradient descent\n",
    "    theta_1 += learning_rate * l1_input.T.dot(delta1)\n",
    "    theta_2 += learning_rate * l2_input.T.dot(delta2)\n",
    "    \n",
    "    #Displays detailed info for each layer's weights at each step\n",
    "    if show_detailed == True:\n",
    "        print(\"-----------------\")\n",
    "        print(\"### Layer 1 ###\\n\")\n",
    "        print(\"Updated Weights:\")\n",
    "        print(theta_1)\n",
    "        print()\n",
    "        print(\"Updated Loss: \")\n",
    "        print(delta1)\n",
    "        print()\n",
    "        \n",
    "        \n",
    "        print(\"-----------------\")\n",
    "        print(\"### Layer 2 ###\\n\")\n",
    "        print(\"Updated Weights:\")\n",
    "        print(theta_2)\n",
    "        print()\n",
    "        print(\"Updated Loss: \")\n",
    "        print(delta2)\n",
    "    \n",
    "    return training_inputs, theta_1, theta_2, pred, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=black>Plotting the Learning Curve</font>\n",
    "Now that we have kept track of our error throughout each iteration, we can measure how well our NN has performed. Looks pretty good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning(error_set):\n",
    "    print(\"----- Learning Curve -----\")\n",
    "    x = list(range(len(error_set)))\n",
    "    plt.title(\"NN Training Error\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.plot(x, error_set, color = 'b')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=black>Putting it all together</font>\n",
    "\n",
    "Now that we have a neural network toolkit, we can now pass the network training training data to test our hypothesis and examine how efficiently the NN can make predictions. I've included two sample training sets, one set for each problem. This will also require us to initialize the weights for the network so that it can start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 0---\n",
      "\n",
      "[[0.76172162]\n",
      " [0.7677612 ]\n",
      " [0.77031712]\n",
      " [0.77131503]]\n",
      "\n",
      "--- Epoch 100---\n",
      "\n",
      "[[0.13878529]\n",
      " [0.17356835]\n",
      " [0.18515116]\n",
      " [0.17692585]]\n",
      "\n",
      "--- Epoch 200---\n",
      "\n",
      "[[0.05483216]\n",
      " [0.18430813]\n",
      " [0.23967759]\n",
      " [0.19562144]]\n",
      "\n",
      "--- Epoch 300---\n",
      "\n",
      "[[0.02996469]\n",
      " [0.19817751]\n",
      " [0.27591442]\n",
      " [0.17316426]]\n",
      "\n",
      "--- Epoch 400---\n",
      "\n",
      "[[0.02297563]\n",
      " [0.20214769]\n",
      " [0.29052675]\n",
      " [0.16336753]]\n",
      "\n",
      "--- Epoch 500---\n",
      "\n",
      "[[0.02074043]\n",
      " [0.20217118]\n",
      " [0.2954716 ]\n",
      " [0.16117507]]\n",
      "\n",
      "--- Epoch 600---\n",
      "\n",
      "[[0.02003829]\n",
      " [0.20178926]\n",
      " [0.29734161]\n",
      " [0.1605706 ]]\n",
      "\n",
      "--- Epoch 700---\n",
      "\n",
      "[[0.01984042]\n",
      " [0.20145178]\n",
      " [0.29817535]\n",
      " [0.1603502 ]]\n",
      "\n",
      "--- Epoch 800---\n",
      "\n",
      "[[0.01980625]\n",
      " [0.20117926]\n",
      " [0.29862497]\n",
      " [0.16024999]]\n",
      "\n",
      "--- Epoch 900---\n",
      "\n",
      "[[0.01982143]\n",
      " [0.20095935]\n",
      " [0.29891442]\n",
      " [0.16019367]]\n",
      "\n",
      "----- Learning Curve -----\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Input ###\n",
      "[[0.1 0.2]\n",
      " [0.4 0.5]\n",
      " [0.6 0.5]\n",
      " [0.8 0.2]]\n",
      "\n",
      "### Expected Output ###\n",
      "[[0.02]\n",
      " [0.2 ]\n",
      " [0.3 ]\n",
      " [0.16]]\n",
      "\n",
      "### Predicted Output ###\n",
      "[[0.01984812]\n",
      " [0.20078125]\n",
      " [0.2991249 ]\n",
      " [0.16015578]]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    The first thing we must do is structure our neural network with the input data in mind,\n",
    "    this includes number of layers, neurons, epochs etc.\n",
    "    \"\"\"   \n",
    "    \n",
    "    ex = 2 #Change this to 2 for problem 2: multiplication\n",
    "    \n",
    "    #Initializing input data sets for exercise 1 & exercise 2\n",
    "    if(ex == 1): #Problem 1 training sets\n",
    "        training_inputs = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "        training_outputs = np.array([[0,1,1,0]]).T\n",
    "    \n",
    "    else: #Problem 2 training sets\n",
    "        #Since I've implemented the sigmoid activation function,I've found that input is limited to postive values\n",
    "        training_inputs = np.array([[.1,.2],[.4,.5],[.6,.5],[.8,.2]])\n",
    "        training_outputs = np.array([[.02,.20,.30,.16]]).T\n",
    "    \n",
    "    #Initializing attributes for NN structure \n",
    "    num_features = training_inputs.shape[1] #initializing number of features/input\n",
    "    hidden_neurons = 2 #initialzing number of neurons in hidden layer\n",
    "    epochs = 1000 #number of iterations\n",
    "    learning_rate = 1 \n",
    "    show_detailed = False #detailed output for each layer's weights\n",
    "    error_set = []\n",
    "    \n",
    "    \"\"\"\n",
    "    Now that we've set up our neural network and structured our training data, we must initialize weights\n",
    "    that the NN will use to start learning with backpropagation.  \n",
    "    \"\"\"\n",
    "    \n",
    "    theta_1 = (np.random.random((num_features + 1, hidden_neurons)))\n",
    "    theta_2 = (np.random.random((hidden_neurons + 1, 1)))\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        training_inputs, theta_1, theta_2, pred, error = back_propagate(training_inputs,training_outputs, theta_1, theta_2, learning_rate, show_detailed)\n",
    "        error_set.append(error)\n",
    "        if(i % 100 == 0):\n",
    "            print(\"--- Epoch \" + str(i) + \"---\\n\")\n",
    "            print(pred)\n",
    "            print()\n",
    "    \n",
    "    l1_input, l1_output, l2_input, l2_output, pred = forward_propagate(training_inputs, theta_1, theta_2) #running forward propagation with the updated weights\n",
    "    \n",
    "    plot_learning(error_set)\n",
    "    \n",
    "    #displaying the predicted output of the model\n",
    "    print(\"### Input ###\")\n",
    "    print(training_inputs)\n",
    "     \n",
    "    print(\"\\n### Expected Output ###\")\n",
    "    print(training_outputs)\n",
    "    \n",
    "    print(\"\\n### Predicted Output ###\")\n",
    "    print(pred)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
